# Performance Improvements Action Plan

**Priority**: Medium  
**Timeline**: Weeks 3-4  
**Status**: 🔴 Not Started

## 🚀 Overview

This plan addresses performance bottlenecks identified in the code review, focusing on parallel processing, caching optimization, and memory management for large codebases.

## 📊 Current Performance Baseline

### Measured Metrics
- **File Processing**: Sequential processing of project files
- **Memory Usage**: Unbounded caching without limits
- **Processing Time**: Linear scaling with project size
- **Cache Efficiency**: No cache hit/miss tracking

### Identified Bottlenecks
1. **Sequential file processing** - No parallelization
2. **Inefficient caching** - No size limits or eviction policies
3. **Memory leaks** - Unbounded data structures
4. **No progress indication** - Poor UX for large projects

## 🎯 Success Criteria

- [ ] 50% reduction in processing time for large projects
- [ ] Memory usage capped at 100MB for typical projects
- [ ] Cache hit rate > 70%
- [ ] Progress indicators for all long-running operations
- [ ] Graceful degradation under memory pressure

## 📝 Detailed Action Items

### Phase 1: Parallel Processing (Week 3)

#### 1.1 Implement Parallel File Processing

**Current State:**
```python
# Sequential processing - slow for large projects
for file_path in project_files:
    rel_fname = str(Path(file_path).relative_to(self.config.project_root))
    try:
        tags = self.repo_map.get_tags(file_path, rel_fname)
        if tags:
            all_tags.extend(tags)
    except Exception as e:
        self.logger.warning(f"Failed to get tags for {rel_fname}: {e}")
```

**Target State:**
```python
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Tuple
import threading

class ParallelTagExtractor:
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
        self._lock = threading.Lock()
    
    def extract_tags_parallel(self, files: List[Path], project_root: Path) -> List[Tag]:
        """Extract tags from files in parallel."""
        all_tags = []
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all file processing tasks
            future_to_file = {
                executor.submit(self._extract_tags_from_file, file, project_root): file 
                for file in files
            }
            
            # Collect results as they complete
            for future in as_completed(future_to_file):
                file = future_to_file[future]
                try:
                    tags = future.result()
                    with self._lock:
                        all_tags.extend(tags)
                except Exception as e:
                    self.logger.error(f"Error processing {file}: {e}")
        
        return all_tags
    
    def _extract_tags_from_file(self, file_path: Path, project_root: Path) -> List[Tag]:
        """Extract tags from a single file."""
        rel_fname = str(file_path.relative_to(project_root))
        try:
            tags = self.repo_map.get_tags(str(file_path), rel_fname)
            return tags or []
        except Exception as e:
            self.logger.warning(f"Failed to get tags for {rel_fname}: {e}")
            return []
```

**Tasks:**
- [ ] Create `ParallelTagExtractor` class
- [ ] Implement thread-safe result collection
- [ ] Add configurable worker count
- [ ] Handle exceptions gracefully in parallel context
- [ ] Add progress tracking for parallel operations

#### 1.2 Add Progress Indicators

**Current State:**
```python
# No progress indication for long operations
project_info = repomap.analyze_project()
```

**Target State:**
```python
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn

def analyze_project_with_progress(self) -> ProjectInfo:
    """Analyze project with progress indication."""
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
        TimeElapsedColumn(),
        console=self.console,
    ) as progress:
        
        # Task 1: Scan files
        scan_task = progress.add_task("Scanning project files...", total=None)
        project_files = self._get_project_files()
        progress.update(scan_task, completed=True)
        
        # Task 2: Extract tags
        extract_task = progress.add_task(
            "Extracting identifiers...", 
            total=len(project_files)
        )
        all_tags = self._extract_tags_parallel(project_files, progress, extract_task)
        
        # Task 3: Analyze results
        analyze_task = progress.add_task("Analyzing results...", total=None)
        project_info = self._create_project_info(all_tags)
        progress.update(analyze_task, completed=True)
        
        return project_info
```

**Tasks:**
- [ ] Integrate Rich progress bars
- [ ] Add progress tracking for parallel operations
- [ ] Implement progress callbacks for long-running tasks
- [ ] Add time estimates for operations
- [ ] Handle progress updates in CLI and API modes

#### 1.3 Optimize File Scanning

**Current State:**
```python
# Inefficient file scanning with repeated path operations
for root, dirs, filenames in os.walk(self.config.project_root):
    # Filter out ignored directories
    dirs[:] = [d for d in dirs if not should_ignore_file(
        Path(root) / d, gitignore_patterns, Path(self.config.project_root)
    )]
```

**Target State:**
```python
class OptimizedFileScanner:
    def __init__(self, project_root: Path, gitignore_patterns: List[str]):
        self.project_root = project_root
        self.gitignore_patterns = gitignore_patterns
        self._ignore_cache = {}
    
    def scan_files(self) -> List[Path]:
        """Scan files with optimized filtering."""
        files = []
        
        for root, dirs, filenames in os.walk(self.project_root):
            # Optimize directory filtering
            dirs[:] = self._filter_directories(root, dirs)
            
            # Process files in batches
            for filename in filenames:
                file_path = Path(root) / filename
                if self._should_include_file(file_path):
                    files.append(file_path)
        
        return files
    
    def _filter_directories(self, root: str, dirs: List[str]) -> List[str]:
        """Filter directories efficiently."""
        return [
            d for d in dirs 
            if not self._is_ignored_directory(Path(root) / d)
        ]
    
    def _is_ignored_directory(self, dir_path: Path) -> bool:
        """Check if directory should be ignored (with caching)."""
        cache_key = str(dir_path)
        if cache_key not in self._ignore_cache:
            self._ignore_cache[cache_key] = should_ignore_file(
                dir_path, self.gitignore_patterns, self.project_root
            )
        return self._ignore_cache[cache_key]
```

**Tasks:**
- [ ] Create `OptimizedFileScanner` class
- [ ] Implement caching for ignore checks
- [ ] Optimize path operations
- [ ] Add batch processing for large directories
- [ ] Profile and optimize hot paths

### Phase 2: Caching & Memory Optimization (Week 4)

#### 2.1 Implement Advanced Caching

**Current State:**
```python
# Simple unbounded cache
self.match_cache: Dict[str, List[Tuple[str, int]]] = {}
```

**Target State:**
```python
from typing import Any, Optional, Tuple
import time
import psutil
from collections import OrderedDict

class AdvancedCacheManager:
    def __init__(self, max_size: int = 1000, max_memory_mb: int = 50):
        self.max_size = max_size
        self.max_memory_bytes = max_memory_mb * 1024 * 1024
        self._cache = OrderedDict()
        self._access_times = {}
        self._size_estimates = {}
    
    def get(self, key: str) -> Optional[Any]:
        """Get value from cache with LRU eviction."""
        if key in self._cache:
            value, timestamp = self._cache[key]
            if time.time() - timestamp < self._get_ttl(key):
                # Update access time for LRU
                self._access_times[key] = time.time()
                self._cache.move_to_end(key)
                return value
            else:
                self._evict(key)
        return None
    
    def put(self, key: str, value: Any, ttl: int = 3600) -> None:
        """Put value in cache with size and memory limits."""
        # Check memory pressure
        if self._should_evict_for_memory():
            self._evict_oldest()
        
        # Check size limits
        if len(self._cache) >= self.max_size:
            self._evict_oldest()
        
        self._cache[key] = (value, time.time())
        self._access_times[key] = time.time()
        self._size_estimates[key] = self._estimate_size(value)
    
    def _should_evict_for_memory(self) -> bool:
        """Check if we need to evict due to memory pressure."""
        current_memory = psutil.Process().memory_info().rss
        return current_memory > self.max_memory_bytes
    
    def _evict_oldest(self) -> None:
        """Evict least recently used items."""
        if not self._cache:
            return
        
        # Find oldest item
        oldest_key = min(self._access_times.keys(), 
                        key=lambda k: self._access_times[k])
        self._evict(oldest_key)
    
    def _evict(self, key: str) -> None:
        """Remove item from cache."""
        if key in self._cache:
            del self._cache[key]
        if key in self._access_times:
            del self._access_times[key]
        if key in self._size_estimates:
            del self._size_estimates[key]
    
    def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics."""
        return {
            "size": len(self._cache),
            "max_size": self.max_size,
            "memory_usage_mb": psutil.Process().memory_info().rss / 1024 / 1024,
            "max_memory_mb": self.max_memory_bytes / 1024 / 1024,
            "hit_rate": self._calculate_hit_rate(),
        }
```

**Tasks:**
- [ ] Implement `AdvancedCacheManager` with memory limits
- [ ] Add cache statistics and monitoring
- [ ] Implement LRU eviction policy
- [ ] Add memory pressure detection
- [ ] Create cache warming strategies

#### 2.2 Memory Usage Monitoring

**Current State:**
```python
# No memory monitoring
def analyze_project(self) -> ProjectInfo:
    # ... analysis logic
    return project_info
```

**Target State:**
```python
import psutil
import time
from dataclasses import dataclass
from typing import Dict, Any

@dataclass
class MemoryMetrics:
    start_memory_mb: float
    peak_memory_mb: float
    end_memory_mb: float
    memory_increase_mb: float

class MemoryMonitor:
    def __init__(self):
        self.process = psutil.Process()
        self.start_memory = None
        self.peak_memory = 0
    
    def __enter__(self):
        self.start_memory = self.process.memory_info().rss / 1024 / 1024
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        current_memory = self.process.memory_info().rss / 1024 / 1024
        self.log_metrics(current_memory)
    
    def check_memory(self) -> float:
        """Check current memory usage and update peak."""
        current_memory = self.process.memory_info().rss / 1024 / 1024
        self.peak_memory = max(self.peak_memory, current_memory)
        return current_memory
    
    def log_metrics(self, end_memory: float) -> None:
        """Log memory usage metrics."""
        metrics = MemoryMetrics(
            start_memory_mb=self.start_memory,
            peak_memory_mb=self.peak_memory,
            end_memory_mb=end_memory,
            memory_increase_mb=end_memory - self.start_memory
        )
        self.logger.info(f"Memory usage: {metrics}")

def analyze_project_with_monitoring(self) -> ProjectInfo:
    """Analyze project with memory monitoring."""
    with MemoryMonitor() as monitor:
        start_time = time.time()
        
        # Monitor memory during analysis
        project_info = self._analyze_project_internal()
        
        analysis_time = time.time() - start_time
        self.logger.info(f"Analysis completed in {analysis_time:.2f}s")
        
        return project_info
```

**Tasks:**
- [ ] Implement `MemoryMonitor` class
- [ ] Add memory usage tracking to all operations
- [ ] Create memory usage alerts
- [ ] Add memory optimization strategies
- [ ] Implement graceful degradation under memory pressure

#### 2.3 Optimize Data Structures

**Current State:**
```python
# Inefficient data structures
all_tags = []  # List with repeated extend operations
identifiers = set()  # Set with repeated add operations
```

**Target State:**
```python
from typing import List, Set, Dict
from collections import defaultdict

class OptimizedDataStructures:
    def __init__(self):
        self.tags_by_file = defaultdict(list)
        self.identifiers_by_type = defaultdict(set)
        self.file_metadata = {}
    
    def add_tags(self, file_path: str, tags: List[Tag]) -> None:
        """Add tags efficiently with file grouping."""
        self.tags_by_file[file_path].extend(tags)
        
        # Update identifier counts by type
        for tag in tags:
            identifier_type = self._classify_identifier(tag.name)
            self.identifiers_by_type[identifier_type].add(tag.name)
    
    def get_all_tags(self) -> List[Tag]:
        """Get all tags efficiently."""
        return [tag for tags in self.tags_by_file.values() for tag in tags]
    
    def get_identifier_stats(self) -> Dict[str, int]:
        """Get identifier statistics efficiently."""
        return {type_name: len(idents) for type_name, idents in self.identifiers_by_type.items()}
    
    def _classify_identifier(self, name: str) -> str:
        """Classify identifier type efficiently."""
        if not name:
            return "unknown"
        if name.isupper():
            return "constants"
        elif name[0].isupper():
            return "classes"
        elif name.endswith("()"):
            return "functions"
        elif name.islower():
            return "variables"
        else:
            return "other"
```

**Tasks:**
- [ ] Implement `OptimizedDataStructures` class
- [ ] Replace inefficient list/set operations
- [ ] Add data structure profiling
- [ ] Optimize identifier classification
- [ ] Implement lazy evaluation where appropriate

## 🔧 Implementation Guidelines

### Performance Testing Framework
```python
import time
import psutil
from dataclasses import dataclass
from typing import Callable, Any

@dataclass
class PerformanceMetrics:
    execution_time: float
    memory_usage_mb: float
    cpu_usage_percent: float
    cache_hit_rate: float

class PerformanceBenchmark:
    def __init__(self):
        self.process = psutil.Process()
    
    def benchmark(self, func: Callable, *args, **kwargs) -> PerformanceMetrics:
        """Benchmark a function's performance."""
        start_time = time.time()
        start_memory = self.process.memory_info().rss / 1024 / 1024
        start_cpu = self.process.cpu_percent()
        
        result = func(*args, **kwargs)
        
        end_time = time.time()
        end_memory = self.process.memory_info().rss / 1024 / 1024
        end_cpu = self.process.cpu_percent()
        
        return PerformanceMetrics(
            execution_time=end_time - start_time,
            memory_usage_mb=end_memory - start_memory,
            cpu_usage_percent=(start_cpu + end_cpu) / 2,
            cache_hit_rate=self._get_cache_hit_rate()
        )
```

### Configuration for Performance
```python
class PerformanceConfig(BaseModel):
    max_workers: int = Field(default=4, ge=1, le=16)
    cache_size: int = Field(default=1000, ge=100, le=10000)
    max_memory_mb: int = Field(default=100, ge=10, le=1000)
    enable_progress: bool = True
    enable_monitoring: bool = True
    parallel_threshold: int = Field(default=10, description="Min files for parallel processing")
```

## 📊 Performance Metrics & Monitoring

### Key Performance Indicators (KPIs)
- **Processing Time**: Target 50% reduction for large projects
- **Memory Usage**: Target <100MB for typical projects
- **Cache Hit Rate**: Target >70%
- **CPU Utilization**: Target <80% during processing
- **Throughput**: Files processed per second

### Monitoring Dashboard
```python
class PerformanceDashboard:
    def __init__(self):
        self.metrics_history = []
    
    def record_metrics(self, metrics: PerformanceMetrics) -> None:
        """Record performance metrics."""
        self.metrics_history.append({
            "timestamp": time.time(),
            "metrics": metrics
        })
    
    def get_summary(self) -> Dict[str, Any]:
        """Get performance summary."""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-10:]  # Last 10 runs
        
        return {
            "avg_execution_time": sum(m["metrics"].execution_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m["metrics"].memory_usage_mb for m in recent_metrics) / len(recent_metrics),
            "avg_cache_hit_rate": sum(m["metrics"].cache_hit_rate for m in recent_metrics) / len(recent_metrics),
            "total_runs": len(self.metrics_history)
        }
```

## 🚀 Rollout Plan

### Week 3: Parallel Processing
- [ ] Implement parallel file processing
- [ ] Add progress indicators
- [ ] Optimize file scanning
- [ ] Performance testing with small projects

### Week 4: Caching & Memory
- [ ] Implement advanced caching
- [ ] Add memory monitoring
- [ ] Optimize data structures
- [ ] Performance testing with large projects

### Week 5: Integration & Validation
- [ ] Integrate all performance improvements
- [ ] End-to-end performance testing
- [ ] Memory leak testing
- [ ] Documentation and monitoring setup

## 📝 Checklist

### Phase 1 Completion Criteria
- [ ] Parallel processing implemented and tested
- [ ] Progress indicators working for all operations
- [ ] File scanning optimized
- [ ] Performance benchmarks established

### Phase 2 Completion Criteria
- [ ] Advanced caching with memory limits
- [ ] Memory monitoring implemented
- [ ] Data structures optimized
- [ ] Performance targets met

## 🔗 Related Documents

- [Critical Issues](./critical-issues.md)
- [Architecture Refactoring](./architecture-refactoring.md)
- [Quality & Testing](./quality-testing.md)

---

**Next Review**: After Phase 2 completion  
**Success Criteria**: 50% performance improvement, memory usage controlled
